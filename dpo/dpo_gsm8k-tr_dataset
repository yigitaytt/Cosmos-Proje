import json
import torch
import re
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

# ============================================================================
# AYARLAR
# ============================================================================
MODEL_PATH = "./merged_model"  #/kaggle/input/sft-lora/pytorch/default/1/final_model ve /kaggle/input/mathmodellarge/pytorch/default/1/final_unwrapped
                                   #modellerinin birleÅŸtirilmiÅŸ hali
OUTPUT_FILE = "dpo_dataset_perfectly_balanced_full.jsonl"
CANDIDATE_COUNT = 4

# Uzunluk ToleransÄ±
MIN_LEN_RATIO = 0.5 
MAX_LEN_RATIO = 1.5

def format_prompt(question):
    # DÃœZELTME: ArtÄ±k prefix eklemiyoruz, sadece soruyu dÃ¶ndÃ¼rÃ¼yoruz.
    return question.strip()

def clean_and_fix_response(text):
    """
    Hem gereksiz 'Cevap: ...' kÄ±sÄ±mlarÄ±nÄ± temizler 
    HEM DE #### iÅŸareti eksikse metnin sonundaki sayÄ±dan kurtarmaya Ã§alÄ±ÅŸÄ±r.
    """
    # 1. Temizlik
    # Model bazen SFT formatÄ±na uygun olarak "### Cevap:" tekrarÄ± yapabilir, onu da temizleyelim
    cleaned = re.sub(r'### Cevap:', '', text, flags=re.IGNORECASE)
    cleaned = re.sub(r'\s*Cevap:\s*-?\d+(\.\d+)?\s*$', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'\s*Answer:\s*-?\d+(\.\d+)?\s*$', '', cleaned, flags=re.IGNORECASE)
    cleaned = cleaned.strip()

    # 2. KURTARMA OPERASYONU
    # GSM8K standardÄ± olan #### iÅŸareti yoksa ekle
    if "####" not in cleaned:
        match = re.search(r'(\d+(?:[\.,]\d+)?)\s*[\.]?$', cleaned)
        if match:
            number = match.group(1)
            cleaned = f"{cleaned} #### {number}"
     
    return cleaned

def remove_calc_tags(text):
    """Metnin iÃ§indeki <<...>> kÄ±sÄ±mlarÄ±nÄ± siler."""
    return re.sub(r'<<.*?>>', '', text)

def remove_newlines(text):
    """Metindeki \n karakterlerini boÅŸluÄŸa Ã§evirir."""
    return text.replace('\n', ', ')

def main():
    print(f"Model YÃ¼kleniyor: {MODEL_PATH}")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        print("SFT_Tokenizer kullanÄ±ldÄ±.")
    except:
        tokenizer = AutoTokenizer.from_pretrained("ytu-ce-cosmos/turkish-gpt2-medium")

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH, 
        torch_dtype=torch.float16, 
        device_map="auto"
    )
    model.eval()

    print("Dataset YÃ¼kleniyor...")
    ds = load_dataset("malhajar/gsm8k-tr", "main", split="train")
    shuffled_ds = ds.shuffle(seed=42)

    total_samples = len(shuffled_ds) # Toplam veri sayÄ±sÄ±nÄ± otomatik al
    
    dpo_data = []
    stats = {
        "success_full": 0,      
        "success_stripped": 0,  
        "success_flattened": 0, 
        "no_hash_tag": 0,       
        "exact_match": 0
    }

    print("ðŸš€ MÃ¼kemmel Dengeli + Tamirli DPO Verisi Ãœretiliyor...")

    for item in tqdm(shuffled_ds):
        question = item['question']
        original_chosen = item['answer'] 
        
        # Formatlama fonksiyonu artÄ±k sadece soruyu dÃ¶ndÃ¼rÃ¼yor
        formatted_prompt = format_prompt(question)
        inputs = tokenizer(formatted_prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.95,
                num_return_sequences=CANDIDATE_COUNT,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
            
        decoded_candidates = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        
        best_rejected = None
        min_len_diff = float("inf")
        
        stripped_chosen = remove_calc_tags(original_chosen)
        flattened_chosen = remove_newlines(original_chosen)
        stripped_flattened_chosen = remove_newlines(stripped_chosen)
        base_chosen_len = len(stripped_flattened_chosen)

        for candidate_text in decoded_candidates:
            # Prompt kÄ±smÄ±nÄ± temizlerken
            if formatted_prompt in candidate_text:
                raw = candidate_text.split(formatted_prompt)[-1].strip()
            else:
                raw = candidate_text.replace(formatted_prompt, "").strip()
            
            rejected_candidate = clean_and_fix_response(raw)
            
            if "####" not in rejected_candidate:
                continue

            if rejected_candidate in [original_chosen, stripped_chosen, flattened_chosen, stripped_flattened_chosen]:
                continue
            
            cand_len = len(rejected_candidate)
            ratio = cand_len / base_chosen_len
            if ratio < MIN_LEN_RATIO or ratio > MAX_LEN_RATIO:
                continue

            diff = abs(cand_len - base_chosen_len)
            if diff < min_len_diff:
                min_len_diff = diff
                best_rejected = rejected_candidate

        if best_rejected:
            final_chosen = original_chosen
            
            if "<<" not in best_rejected:
                final_chosen = remove_calc_tags(final_chosen)
                stats["success_stripped"] += 1
            else:
                stats["success_full"] += 1

            if "\n" not in best_rejected:
                final_chosen = remove_newlines(final_chosen)
                stats["success_flattened"] += 1

            dpo_data.append({
                "prompt": formatted_prompt,
                "chosen": final_chosen,
                "rejected": best_rejected
            })
        else:
            stats["no_hash_tag"] += 1

    print("\n" + "="*50)
    print(f"Toplam Denenen: {total_samples}")
    print(f"âœ… Toplam BaÅŸarÄ±lÄ± Veri: {len(dpo_data)}")
    print(f"â„¹ï¸  Detaylar:")
    print(f"   - Tam FormatlÄ±: {stats['success_full']}")
    print(f"   - TagÄ± Silinenler: {stats['success_stripped']}")
    print(f"   - DÃ¼zleÅŸtirilenler: {stats['success_flattened']}")
    print(f"âŒ KurtarÄ±lamayan: {stats['no_hash_tag']}")
    print("="*50)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for entry in dpo_data:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            
    print(f"Dosya kaydedildi: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
