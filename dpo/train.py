import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig

# --- 1. AYARLAR ---
model_name = "/kaggle/input/mathmodellarge/pytorch/default/1/final_unwrapped"  #kaggle'dan Ã§ekilmiÅŸ pre-train edilmiÅŸ model
new_model_name = "uhem-dpo-model"

# --- 2. MODEL VE TOKENIZER ---
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,    #torch_dtype=dtype olarak deÄŸiÅŸebilir.
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token 
tokenizer.padding_side = "left" # DPO iÃ§in sol padding ÅART

# --- 3. VERÄ° SETÄ° VE FORMATLAMA (GÃœNCELLENDÄ°) ---
# Veriyi yÃ¼klÃ¼yoruz
dataset = load_dataset("json", data_files="dpo_data.json", split="train")   #dataset henÃ¼z belli deÄŸil


def format_for_sft_style(example):
    # 1. Ham soruyu al (EÄŸer sÃ¼tun adÄ± 'instruction' veya 'question' ise burayÄ± deÄŸiÅŸtir)
    # Genelde DPO datasetlerinde 'prompt' olur.
    raw_prompt = example.get("prompt") or example.get("question") or example.get("instruction")
    
    # 2. Modelin SFT'de alÄ±ÅŸtÄ±ÄŸÄ± kÄ±yafeti giydir
    # Model "### Answer:" gÃ¶rmeden cevap vermez!
    formatted_prompt = f"### Question:\n{raw_prompt}\n\n### Answer:\n"
    
    return {
        "prompt": formatted_prompt,      # ArtÄ±k etiketli!
        "chosen": example["chosen"],     # Cevaplara dokunmuyoruz
        "rejected": example["rejected"]  # Cevaplara dokunmuyoruz
    }

# Dataseti bu fonksiyonla gÃ¼ncelliyoruz
print("Veri seti SFT formatÄ±na (### Question...) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
dataset = dataset.map(format_for_sft_style)

# KONTROL (Ä°Ã§inin rahat etmesi iÃ§in ilk veriyi basÄ±yoruz)
print(f"--- Ã–RNEK GÄ°RDÄ° ---\n{dataset[0]['prompt']}")

# --- 4. LORA AYARLARI ---
peft_config = LoraConfig(
    r=32,      # nxr , rxn formatÄ±nda 2 matris oluÅŸacak
    lora_alpha=64,
    lora_dropout=0.05,   #aÄŸÄ±rlÄ±k matrisinde Ã¼zeri kapatÄ±lÄ±p 0 yapÄ±lan deÄŸerlerin oranÄ±. 
                         #Overfiti (modelin, acaba kelimesinden sonra 4 gelmesini ezberlemesini) engeller. 15.000 satÄ±rlÄ±k bir veri seti iÃ§in 0.05 iyidir.
                         #Veri seti boyutu arttÄ±kÃ§a bu deÄŸer kÃ¼Ã§Ã¼lmelidir. Zaten overfit olma durumu bÃ¼yÃ¼k veri setlerinde dÃ¼ÅŸÃ¼ktÃ¼r.
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["c_attn", "c_proj", "c_fc"] 
)

# --- 5. EÄÄ°TÄ°M KONFÄ°GÃœRASYONU ---
training_args = DPOConfig(
    output_dir="./dpo_results",
    beta=0.1,      #base modelden ne kadar uzaklaÅŸacaÄŸÄ±mÄ±zÄ± ayarlar. 0.1 genellikle standarttÄ±r.
    
    # --- Performans ---
    learning_rate=5e-6,      #DPO algoritmasÄ± iÃ§in 5e-6 1e-7 gibi kÃ¼Ã§Ã¼k lr deÄŸerleri daha optimizedir.
    num_train_epochs=1,      #Veri setinde tek tur eÄŸitim yapÄ±lÄ±r.
    per_device_train_batch_size=2,   # Bir seferde kaÃ§ satÄ±rlÄ±k veri Ã¼zerinde Ã§alÄ±ÅŸacaÄŸÄ±mÄ±zÄ± belirtir. 
    gradient_accumulation_steps=8,  # 2x8 = 16, 8 adÄ±m sonrasÄ±nda model ortalama alarak gÃ¼ncelleme yapar.
    
    # --- IsÄ±nma (Warm-up) ---
    warmup_ratio=0.05,    #BaÅŸlangÄ±Ã§ta momentum olmadÄ±ÄŸÄ± iÃ§in SFT'den gelen aÄŸÄ±rlÄ±klarda yÃ¼ksek bir deÄŸiÅŸiklik yapmamasÄ± iÃ§in yavaÅŸ yavaÅŸ modelin eÄŸitilmesini saÄŸlar.
                         #Veri setinin ilk 0.05 oranÄ±ndaki adÄ±mÄ±nda model yavaÅŸ ÅŸekilde eÄŸitilir. LR deÄŸeri bundan sonra tam deÄŸerinde kullanÄ±lÄ±r.
  
    lr_scheduler_type="cosine",  #Modelin en sonda yavaÅŸ yavaÅŸ deÄŸiÅŸikliÄŸi bitirmesini saÄŸlar.
                                 # EÄŸer model sonlarda optimum hale geldiyse modelde fazla deÄŸiÅŸiklik yapmamÄ±zÄ±n Ã¶nÃ¼ne geÃ§miÅŸ olur.
    logging_steps=10,
    save_steps=100,
    fp16=True,
    optim="paged_adamw_32bit",
    remove_unused_columns=False 
)

# --- 6. TRAINER BAÅLATMA ---
trainer = DPOTrainer(
    model=model,
    ref_model=None,
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer, 
    peft_config=peft_config,
    max_prompt_length=512,
    max_length=1024,
)

# --- 7. BAÅLAT ---
print("ğŸš€ DPO EÄŸitimi (FormatlÄ± ve GÃ¼venli) BaÅŸlÄ±yor...")
trainer.train()

# --- 8. KAYDET ---
trainer.model.save_pretrained(new_model_name)
tokenizer.save_pretrained(new_model_name)
print(f"âœ… Model {new_model_name} klasÃ¶rÃ¼ne kaydedildi!")
