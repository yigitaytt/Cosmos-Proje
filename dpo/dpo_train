import torch
import os
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig

# ==========================================
# 1. AYARLAR VE YOLLAR
# ==========================================
MODEL_PATH = "/kaggle/working/merged_model"
DATASET_PATH = "/kaggle/working/dpo_dataset_final.jsonl"
NEW_MODEL_NAME = "dpo_final_model"

dtype = torch.float16 if torch.cuda.is_available() else torch.float32

print(f" Model yÃ¼kleniyor: {MODEL_PATH}")

# ==========================================
# 2. MODEL VE TOKENIZER (Ä°KÄ° MODEL YÃœKLE!)
# ==========================================
# Policy model (eÄŸitilecek)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=dtype,
    device_map="auto"
)

# Reference model (frozen, eÄŸitilmeyecek)  YENÄ°!
print("ğŸ“¥ Reference model yÃ¼kleniyor...")
ref_model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=dtype,
    device_map="auto"
)
ref_model.requires_grad_(False)  # Freeze
ref_model.eval()  # Eval mode

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

print(" Model, Reference Model ve Tokenizer hazÄ±r.")

# ==========================================
# 3. VERÄ° SETÄ°
# ==========================================
print(f"ğŸ“‚ Veri seti yÃ¼kleniyor: {DATASET_PATH}")
dataset = load_dataset("json", data_files=DATASET_PATH, split="train")

def format_prompt(example):
    example["prompt"] = f"### Soru: {example['prompt'].strip()}\n### Cevap:"
    return example

print("ğŸ› ï¸ Promptlar formatlanÄ±yor...")
dataset = dataset.map(format_prompt)

dataset_split = dataset.train_test_split(test_size=0.05, seed=42)
train_dataset = dataset_split["train"]
eval_dataset = dataset_split["test"]

print(f"ğŸ“Š EÄŸitim: {len(train_dataset)} | Test: {len(eval_dataset)}")

# ==========================================
# 4. LORA
# ==========================================
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["c_attn", "c_proj", "c_fc"]
)

# ==========================================
# 5. DPO CONFIG (DÃœZELTÄ°LMÄ°Å PARAMETRELER!)
# ==========================================
training_args = DPOConfig(
    output_dir="./dpo_results",
    
    # DPO Parametreleri - DÃœZELTÄ°LDÄ° 
    beta=0.2,                    # 0.1 â†’ 0.2 (daha gÃ¼Ã§lÃ¼ sinyal)
    loss_type="sigmoid",
    
    # Learning Rate - DÃœZELTÄ°LDÄ° 
    learning_rate=5e-7,          # 5e-6 â†’ 5e-7 (10x daha dÃ¼ÅŸÃ¼k)
    num_train_epochs=1,
    
    # Batch Size - DÃœZELTÄ°LDÄ° 
    per_device_train_batch_size=1,    # 2 â†’ 1
    gradient_accumulation_steps=32,   # 8 â†’ 32 (effective=32)
    
    # Raporlama
    eval_strategy="steps",
    eval_steps=50,
    save_steps=100,
    logging_steps=10,
    
    # Optimizasyon
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    fp16=True,
    optim="paged_adamw_32bit",
    
    # Uzunluk
    max_prompt_length=512,
    max_length=1024,
    remove_unused_columns=False
)

# ==========================================
# 6. TRAINER - 
# ==========================================
print("\n DPO EÄŸitimi BaÅŸlÄ±yor...")
print("  Ä°zlenmesi Gerekenler:")
print("   - rewards/chosen: ArtmalÄ± (model doÄŸru cevaba yÃ¶nelmeli)")
print("   - rewards/rejected: AzalmalÄ± (model yanlÄ±ÅŸtan uzaklaÅŸmalÄ±)")
print("   - rewards/margins: ArtmalÄ± (fark aÃ§Ä±lmalÄ±)")
print("-" * 50)

trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,  #  None yerine aÃ§Ä±kÃ§a belirt
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    processing_class=tokenizer,
    peft_config=peft_config,
)

trainer.train()

# ==========================================
# 7. KAYDETME
# ==========================================
print(f"\nğŸ’¾ Model kaydediliyor: {NEW_MODEL_NAME}")
trainer.model.save_pretrained(NEW_MODEL_NAME)
tokenizer.save_pretrained(NEW_MODEL_NAME)

print("DPO EÄŸitimi TamamlandÄ±!")

# ==========================================
# 8. TEST
# ==========================================
def test_model(prompt_text):
    full_prompt = f"### Soru: {prompt_text}\n### Cevap:"
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print("\n Test YapÄ±lÄ±yor...")
try:
    raw_prompt = eval_dataset[0]['prompt'].replace("### Soru: ", "").replace("\n### Cevap:", "").strip()
    print(f"Soru: {raw_prompt}")
    print("-" * 20)
    print(f"Cevap:\n{test_model(raw_prompt)}")
except Exception as e:
    print(f"Hata: {e}")
```

---

## ğŸ“Š **Training Log'larda Bakman Gerekenler**

EÄŸitim sÄ±rasÄ±nda ÅŸunlarÄ± izle:
```
train/rewards/chosen:    0.5 â†’ 1.2 â†’ 2.0   ArtmalÄ±
train/rewards/rejected: -0.3 â†’ -1.0 â†’ -2.5   AzalmalÄ±
train/rewards/margins:   0.8 â†’ 2.2 â†’ 4.5   ArtmalÄ± (fark aÃ§Ä±lmalÄ±)
train/loss:              0.69 â†’ 0.45 â†’ 0.30   AzalmalÄ±
